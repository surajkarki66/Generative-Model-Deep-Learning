{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Generative Adversarial Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (None, 28, 28, 1), types: tf.float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating The Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Reshape((7, 7, 256))) # 7 x 7 x 256\n",
    "    \n",
    "    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)) # 14 x14 x 128\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)) # 28 x 28 x 64\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')) # 28 x 28 x 1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12544)             1254400   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 7, 7, 128)         819200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 64)        204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         1600      \n",
      "=================================================================\n",
      "Total params: 2,330,944\n",
      "Trainable params: 2,305,472\n",
      "Non-trainable params: 25,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = generator_model()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 100])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise = tf.random.normal([1, 100])\n",
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6c336e0c40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_image = generator(noise, training=False)\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(tf.keras.layers.LeakyReLU())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.00086642]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator loss\n",
    "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.1390165, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "real_output = np.array([1, 1, 1, 1, 1], dtype=np.float32)\n",
    "fake_output = np.array([0.3, 0.3, 0.2, 0.34, 0.1], dtype=np.float32)\n",
    "d_l = discriminator_loss(real_output, fake_output)\n",
    "print(d_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Loss\n",
    "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss,disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        gen_loss_list = []\n",
    "        disc_loss_list = []\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            t = train_step(image_batch)\n",
    "            gen_loss_list.append(t[0])\n",
    "            disc_loss_list.append(t[1])\n",
    "        \n",
    "        g_loss = sum(gen_loss_list) / len(gen_loss_list)\n",
    "        d_loss = sum(disc_loss_list) / len(disc_loss_list)\n",
    "        # Produce images for the GIF as we go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator,\n",
    "                                 epoch + 1,\n",
    "                                 seed)\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "        print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss}')\n",
    "\n",
    "   # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "    # Notice `training` is set to False.\n",
    "    # This is so all layers run in inference mode (batchnorm).\n",
    "    predictions = model(test_input, training=False)\n",
    "\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
