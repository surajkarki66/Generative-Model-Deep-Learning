## GAN(Generative Adversarial Network)

#### Introduction

First GAN is created by Ian Goodfellow in 2014. It is a one of the famous generative model.
The Generative Adversarial Network (GAN) comprises of two models: a generative model G and a discriminative model D. The generative model can be considered as a counterfeiter who is trying to generate fake currency and use it without being caught, whereas the discriminative model is similar to police, trying to catch the fake currency. This competition goes on till the counterfeiter becomes smart enough to successfully fool the police.

It is called Adversarial because it is a competition between two components.

#### Generator:

It takes a sample data from input noise and generate a fake data exactly like real input data.
The generator model learns the joint probability distribution of the input variable and output variable.
It uses Naive Bayes .
`-> P(X, Y)`
`-> P(X|Y) P(Y)`
`-> P(Y|X) P(X)`

#### Discriminator:

It tries to predict whether the data generated by generator is real or fake.
The models learns conditional probability of the targeted variable given the input variable.
eg: Logistic Regression.
`-> P(Y|X=x)`

#### Lets define some parameter and variable

![1_No6f6hSCKaiXs2Mr9T9wpA](https://user-images.githubusercontent.com/50628520/90130639-8718db80-dd8a-11ea-81ed-1e4e0363c1af.png)

#### Structure of a GAN

![st](https://user-images.githubusercontent.com/50628520/90130850-dd861a00-dd8a-11ea-9d2b-3f1ed7e3dd88.jpg)

Here Discriminator try to maximize the chances of predicting classes.
But Generator try to fool Discriminator by producing data same as input data.

#### Loss Function:

![1_kACQPgX3Dk-X7u3J51JXxw](https://user-images.githubusercontent.com/50628520/90130948-07d7d780-dd8b-11ea-9301-8f5c92451ef4.png)

The loss function described in the original paper by Ian Goodfellow et al. can be derived from the formula of binary cross-entropy loss. The binary cross-entropy loss can be written as,

##### 1) Discriminator loss:

![1_brURhcYCI6WtbTuEw1XJ-Q](https://user-images.githubusercontent.com/50628520/90131095-47062880-dd8b-11ea-938e-523698240d8e.png)

Now, the objective of the discriminator is to correctly classify the fake and real dataset. For this, equations (1) and (2) should be maximized and final loss function for the discriminator can be given as,
![1_fWHhgZg_KNS3h8qq82QD3Q](https://user-images.githubusercontent.com/50628520/90131135-5be2bc00-dd8b-11ea-8c87-a53ef70fde51.png)

##### 2) Generator loss:

Here, the generator is competing against discriminator. So, it will try to minimize the equation (3) and loss function is given as

![1_Hdgo1Slf4fDiSrUfO3IwIQ](https://user-images.githubusercontent.com/50628520/90131385-c693f780-dd8b-11ea-9b25-c6495bfd2e1e.png)

##### 3) Combined loss function:

Combining eqn 3 and 4 we get.

![1__vFj5-pJP866w-LzdaHdoA](https://user-images.githubusercontent.com/50628520/90131458-e75c4d00-dd8b-11ea-86c2-df04ec7e493d.png)

Remember that the above loss function is valid only for a single data point, to consider entire dataset we need to take the expectation of the above equation as

![1_79sBUnY8G3nlV9khUr1jkg](https://user-images.githubusercontent.com/50628520/90131532-0529b200-dd8c-11ea-9340-5ee41b24b3b4.png)

#### Training Loop:

![p](https://user-images.githubusercontent.com/50628520/90131762-65205880-dd8c-11ea-8bb5-410dbe1e9e88.jpg)

It can be noticed from the above algorithm that the generator and discriminator are trained separately. In the first section, real data and fake data are inserted into the discriminator with correct labels and training takes place. Gradients are propagated keeping generator fixed. Also, we update the discriminator by ascending its stochastic gradient because for discriminator we want to maximize the loss function given in equation (6).
On the other hand, we update the generator by keeping discriminator fixed and passing fake data with fake labels in order to fool the discriminator. Here, we update the generator by descending its stochastic gradient because for the generator we want to minimize the loss function given in equation (6).

#### Generated by mnist dcgan

![dcgan (1)](https://user-images.githubusercontent.com/50628520/90311344-b90c7800-df19-11ea-8131-2fbaf307620f.gif)

#### Generated by anime dcgan

![animedcgan](https://user-images.githubusercontent.com/50628520/90311410-510a6180-df1a-11ea-9097-b328d3881360.gif)
