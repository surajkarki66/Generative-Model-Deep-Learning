{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation, Dense, Input\n",
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical, plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 28\n",
    "width = 28\n",
    "n_classes = 10\n",
    "latent_dim = 100\n",
    "n_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building CGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    image_resize = height // 4\n",
    "\n",
    "    input_data = Input(shape=(latent_dim,), name='z_input')\n",
    "    input_labels = Input(shape=(n_classes,), name='class_labels')\n",
    "\n",
    "    x = concatenate([input_data, input_labels], axis=1)\n",
    "    x = Dense(image_resize * image_resize * 128)(x)\n",
    "    x = Reshape((image_resize, image_resize, 128))(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same')(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same')(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(filters=32, kernel_size=5, strides=1, padding='same')(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2DTranspose(filters=1, kernel_size=5, strides=1, padding='same')(x)\n",
    "\n",
    "    x = Activation('sigmoid')(x)\n",
    "    \n",
    "    # input is conditioned by labels\n",
    "    generator = Model(inputs=[input_data, input_labels], outputs=x, name='generator')\n",
    "    return generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "z_input (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "class_labels (InputLayer)       [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 110)          0           z_input[0][0]                    \n",
      "                                                                 class_labels[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 6272)         696192      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 7, 7, 128)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 7, 7, 128)    512         reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 7, 7, 128)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 14, 14, 128)  409728      activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 14, 14, 128)  512         conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 14, 14, 128)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 28, 28, 64)   204864      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 28, 28, 64)   256         conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 28, 28, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 28, 28, 32)   51232       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 28, 28, 32)   128         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 28, 28, 32)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 28, 28, 1)    801         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 28, 28, 1)    0           conv2d_transpose_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 1,364,225\n",
      "Trainable params: 1,363,521\n",
      "Non-trainable params: 704\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    # input image\n",
    "    model_input = Input(shape=(width, height, n_channels), name='discriminator_input')\n",
    "    x = model_input\n",
    "    # input label\n",
    "    labels = Input(shape=(n_classes,))\n",
    "    \n",
    "    labels_embedded = Dense(width * width)(labels)\n",
    "    labels_embedded = Reshape((width, height, n_channels))(labels_embedded)\n",
    "\n",
    "    x = concatenate([x, labels_embedded])\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=5, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=5, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    x = Conv2D(filters=128, kernel_size=5, strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=5, strides=1, padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    # model_input is conditioned by labels\n",
    "    discriminator = Model([model_input, labels], x, name='discriminator')\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 784)          8624        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "discriminator_input (InputLayer [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 28, 28, 1)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 28, 28, 2)    0           discriminator_input[0][0]        \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 28, 28, 2)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 14, 14, 32)   1632        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 14, 14, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 7, 7, 64)     51264       leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 7, 7, 64)     0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 4, 4, 128)    204928      leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 4, 4, 128)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 4, 256)    819456      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 256)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 4096)         0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            4097        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,090,001\n",
      "Trainable params: 1,090,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noise(type_of_noise, batch_size):\n",
    "    if type_of_noise == \"normal_noise\":\n",
    "        return np.random.normal(0, 1, size=[batch_size, latent_dim])\n",
    "\n",
    "    elif type_of_noise == \"uniform_noise\":\n",
    "        return np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch):\n",
    "    r, c = 2, 5\n",
    "    noise = np.random.uniform(-1.0, 1.0, size=[r * c, latent_dim])\n",
    "\n",
    "    sampled_labels = np.arange(0, 10).reshape(-1, 1)\n",
    "    sampled_labels_categorical = to_categorical(sampled_labels)\n",
    "\n",
    "    gen_imgs = generator.predict([noise, sampled_labels_categorical])\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    # gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "            axs[i, j].set_title(\"Digit: %d\" % sampled_labels[cnt])\n",
    "            axs[i, j].axis('off')\n",
    "            cnt += 1\n",
    "    fig.savefig(\"images/%d.png\" % epoch, bbox_inches='tight', dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(loss=['binary_crossentropy'], optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = Input(shape=(latent_dim, ))\n",
    "label = Input(shape=(n_classes,))\n",
    "img = generator([noise, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# during generator updating,  the discriminator is fixed (will not be updated).\n",
    "discriminator.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The discriminator takes generated image and label as input and determines its validity\n",
    "validity = discriminator([img, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan_model = Model(inputs=[noise, label], outputs=validity)\n",
    "cgan_model.compile(loss=['binary_crossentropy'],optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "generator (Model)               (None, 28, 28, 1)    1364225     input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "discriminator (Model)           (None, 1)            1090001     generator[1][0]                  \n",
      "                                                                 input_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,454,226\n",
      "Trainable params: 1,363,521\n",
      "Non-trainable params: 1,090,705\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cgan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " def train(x_train, y_train, epochs=1000, batch_size=128, sample_interval=50):\n",
    "        x_train = np.reshape(x_train, [-1, width, height, n_channels])\n",
    "        x_train = x_train.astype('float32') / 255\n",
    "\n",
    "        y_train = to_categorical(y_train)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        real = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            #  --------------------- Train Discriminator ---------------------\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, x_train.shape[0], size=batch_size)\n",
    "            imgs, labels = x_train[idx], y_train[idx]\n",
    "\n",
    "            # Generate sample noise for generator input\n",
    "            noise = generate_noise(\"uniform_noise\", batch_size)\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            # we can use labels instead of fake_labels; because it is fake for noise\n",
    "            gen_imgs = generator.predict([noise, labels])\n",
    "\n",
    "            # --------------------- Train the Discriminator ---------------------\n",
    "            d_loss_real = discriminator.train_on_batch([imgs, labels], real)\n",
    "            d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            #  --------------------- Train the Generator ---------------------\n",
    "            # Condition on labels (random one-hot labels)\n",
    "            fake_labels = np.eye(n_classes)[np.random.choice(n_classes, batch_size)]\n",
    "\n",
    "            # Train the generator\n",
    "            cgan_loss, acc = cgan_model.train_on_batch([noise, fake_labels], real)\n",
    "\n",
    "            # Plot the progress\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], cgan_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                sample_images(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(X, y, epochs=1000, batch_size=32, sample_interval=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_weights(\"weights/generator.h5\")\n",
    "discriminator.load_weights(\"weights/discriminator.h5\")\n",
    "cgan_model.load_weights(\"weights/cgan.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.uniform(-1.0, 1.0, size=[1, latent_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_labels = np.arange(0, 10).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_labels_categorical = to_categorical(sampled_labels)\n",
    "label = np.array([sampled_labels_categorical[3]], dtype=np.float32)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img = generator.predict([noise, label])\n",
    "disc = discriminator.predict([gen_img, label])\n",
    "cgan = cgan_model.predict([noise, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_img.shape\n",
    "disc.shape\n",
    "cgan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "img  = gen_img * 255\n",
    "img = img.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f09247804c0>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    sample_images(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
